---
title: '**SelvarMix**'
output: html_document
bibliography: biblio.bib
---

*SelvarMix: A R package for variable selection in model-based clustering and discriminant analysis with a regularization approach.*

**Description:**

* *Authors*: **Gilles Celeux** and **Cathy Maugis-Rabusseau** and **Mohammed A. Sedki**.
* *License*: [GPL-2](http://www.gnu.org/licenses/gpl-2.0.html).
* *Download SelvarMix 1.2 (beta version)*: [link](http://r-forge.r-project.org/R/?group_id=2044). 

*  *Download SelvarMix 1.1 (cran version)*: [link](http://cran.r-project.org/web/packages/SelvarMix/).
* *Reference*: [SelvarMix: A R package for variable selection in model-based clustering and discriminant analysis with a regularization approach](https://hal.archives-ouvertes.fr/hal-01053784/), Celeux, G., and Maugis-Rabusseau, C. and Sedki, M. 2016, preprint.


<a id="top"></a>
**Site map:**  

* <a href="#intro">Introduction</a>.
* <a href="#dataset">Synthetic dataset</a>.
* <a href="#clust">Variable selection in model-based clustering</a>.
* <a href="#discrim">Variable selection in classification</a>.

All the experiments are implemented with SelvarMix 1.2
<a id="intro"></a>

### Introduction
*SelvarMix* package carries out a regularization approach of variable selection in the model-based clustering and classification 
frameworks. First, the
variables are arranged in order with a lasso-like procedure. Second, the method
of [@Maugis2009b; @Maugis2011] is adapted to define the role of variables in the
two frameworks. This variable ranking allows us to avoid the painfully slow
stepwise algorithms of [@Maugis2009b]. Thus, SelvarMix provides a much 
faster variable selection procedure than [@Maugis2009b;@Maugis2011] and allows 
us to study high-dimensional datasets.


Tool functions *summary and print*  facilitate the result interpretation.





###  Overview of the SelvarMix functions
This section performs the whole analysis of a simulated  data set . 
It uses all the functions implemented in the package *SelvarMix* and can be used as a tutorial. 

The cluster analysis is performed with unknown number of clusters. We use an information criterion for variable
selection and choosing the number of clusters.  To display the winning model, the summary is given. 

**The synthetic dataset**

 The simulated dataset consists of 2000 data points in $\mathbb{R}^{14}$. 
 On the subset of relevant clustering variables $S = \{1, 2\}$, 
 data are distributed from a mixture of four equiprobable 
 spherical Gaussian distributions with means $(0,0), (3,0)
 (0,3)$ and $(3,3)$. The subset of redundant variables 
 is $U =\{3-11\}$ that are explained by the subset of 
 predictor variables $R = \{1,2\}$. 
 The last three variables are independent $W = \{12, 13, 14\}$. More details are given in [@Maugis2009b].

<a id="tutorial"></a>
```{r, comment=""}
set.seed(123)
n <- 2000; p <- 14
x <- matrix(0,n, p)
x[,1] <- rnorm(n,0,1)
x[,2] <- rnorm(n,0,1)
z <-  sample(1:4, n, rep=T)
x[z==2, 1] <- x[z==2, 1] + 3
x[z==3, 2] <- x[z==3, 2] + 3
x[z==4, 1] <- x[z==4, 1] + 3
x[z==4, 2] <- x[z==4, 2] + 3

omega <- matrix(0, 9, 9); diag(omega)[1:3] <- rep(1,3); diag(omega)[4:5] <- rep(0.5,2)
rtmat1 <- matrix(c(cos(pi/3), -sin(pi/3), sin(pi/3), cos(pi/3)), ncol = 2, byrow = TRUE)
rtmat2 <- matrix(c(cos(pi/6), -sin(pi/6), sin(pi/6), cos(pi/6)), ncol = 2, byrow = TRUE)
omega[6:7, 6:7] <- t(rtmat1) %*% diag(c(1,3)) %*% rtmat1
omega[8:9, 8:9] <- t(rtmat2) %*% diag(c(2,6)) %*% rtmat2
b <- cbind(c(0.5,1), c(2,0), c(0,3), c(-1,2), c(2,-4), c(0.5,0), c(4,0.5), c(3,0), c(2,1))
x[,3:11] <- c(0, 0, seq(0.4, 2, len=7)) + x[,1:2]%*%b + t(t(chol(omega)) %*% matrix(rnorm(n*9), 9, n)) 
x[,12:14] <- matrix(rnorm(3*n), n, 3)
x[,12] <- x[,12] + 3.2; x[,13] <- x[,13] + 3.6; x[,13] <- x[,13] + 4
```
* <a href="#top">Go to the top</a>

<a id="clust"></a>
**Variable selection and selection of the number of clusters in the clustering framework**
```{r, comment=""}
# Cluster analysis with variable selection with parallel computing (8 cores) 
# The last two input arguments are optional
require(SelvarMix)
obj <- SelvarClustLasso(x=x, nbcluster=3:5, models=mixmodGaussianModel(family = "spherical"), nbcores=8)
```

**Model Summary**

```{r, comment=""}
# Summary of the selected model
summary(obj)
```

* <a href="#top">Go to the top</a>

**Result print**
```{r, comment=""}
# print clustering and regression parameters 
print(obj)
```
* <a href="#top">Go to the top</a>

<a id="discrim"></a>
**Discriminant analysis with variable selection**
```{r, comment=""}
# Discriminant analysis with learning and testing data
# Variable selection with parallel computing (8 cores)
xl <- x[1:1900,]; xt <- x[1901:2000,] 
zl <- z[1:1900]; zt <- z[1901:2000]
obj <- SelvarLearnLasso(x=xl, z=zl, models=mixmodGaussianModel(family = "spherical"), xtest=xt, ztest=zt,nbcores=8)
```
**Model Summary**

```{r, comment=""}
# Summary of the selected model
summary(obj)
```
* <a href="#top">Go to the top</a>

**Result print**
```{r, comment=""}
# print clustering and regression parameters 
print(obj)
```
* <a href="#top">Go to the top</a>

